{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium networkx torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHMy8---NBT2",
        "outputId": "dc4e8017-771f-4cb5-d988-3f4bbee60bc8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.14)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "fTKVHG0BeAV-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GraphColoringEnv(gym.Env):\n",
        "    def __init__(self, num_colors=3):\n",
        "        super(GraphColoringEnv, self).__init__()\n",
        "        self.num_colors = num_colors\n",
        "        self.edge_index = self._create_edge_index()\n",
        "        self.n_nodes = int(torch.max(self.edge_index)) + 1\n",
        "\n",
        "        # Action space: (node_id, color_id)\n",
        "        self.action_space = spaces.MultiDiscrete([self.n_nodes, self.num_colors])\n",
        "\n",
        "        # Observation: PyG Data object with x=[node_color] and edge_index\n",
        "        self.observation_space = None  # not used in PyG; handled by downstream model\n",
        "\n",
        "        self.state = None\n",
        "        self.reset()\n",
        "\n",
        "    def _create_edge_index(self):\n",
        "        # Define a fixed graph: (0-1-2 triangle), 1-3-4 chain\n",
        "        edges = [\n",
        "            [0, 1], [1, 0],\n",
        "            [0, 2], [2, 0],\n",
        "            [1, 2], [2, 1],\n",
        "            [1, 3], [3, 1],\n",
        "            [3, 4], [4, 3]\n",
        "        ]\n",
        "        return torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        # -1 for all uncolored nodes\n",
        "        self.state = torch.full((self.n_nodes, 1), -1, dtype=torch.long)\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return Data(x=self.state.clone(), edge_index=self.edge_index)\n",
        "\n",
        "    def step(self, action):\n",
        "        node, color = action\n",
        "        node = int(node)\n",
        "        color = int(color)\n",
        "\n",
        "        reward = 0\n",
        "        done = False\n",
        "\n",
        "        if self.state[node].item() != -1:\n",
        "            print(\"Node already colored\")\n",
        "            reward = -1  # Node already colored\n",
        "        else:\n",
        "            # Check for conflict with neighbors\n",
        "            neighbors = self.edge_index[1][self.edge_index[0] == node]\n",
        "            neighbor_colors = self.state[neighbors]\n",
        "            if (neighbor_colors == color).any():\n",
        "                reward = -1\n",
        "            else:\n",
        "                self.state[node] = color\n",
        "                reward = 1\n",
        "\n",
        "        # Check if done (all nodes colored)\n",
        "        done = (self.state != -1).all().item()\n",
        "        if done and reward > 0:\n",
        "            reward += 10  # Bonus for successful coloring\n",
        "\n",
        "        return self._get_obs(), reward, done, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(\"Node Colors:\", self.state.view(-1).tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = GraphColoringEnv(num_colors=3)\n",
        "obs, _ = env.reset()\n",
        "\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    # Choose a random uncolored node and a random color\n",
        "    uncolored = (obs.x.view(-1) == -1).nonzero(as_tuple=True)[0]\n",
        "    if len(uncolored) == 0:\n",
        "        break\n",
        "    node = uncolored[0].item()\n",
        "    color = np.random.randint(0, env.num_colors)\n",
        "    obs, reward, done, _, _ = env.step((node, color))\n",
        "    total_reward += reward\n",
        "\n",
        "print(\"\\nFinal Colors:\", obs.x.view(-1).tolist())\n",
        "print(\"Total Reward:\", total_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjMHrLQIeQBq",
        "outputId": "93cfd3f0-4457-43f1-f9e0-53f8e8dcb7c3"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Colors: [-1, -1, -1, -1, -1]\n",
            "Node Colors: [1, -1, -1, -1, -1]\n",
            "Node Colors: [1, -1, -1, -1, -1]\n",
            "Node Colors: [1, 0, -1, -1, -1]\n",
            "Node Colors: [1, 0, -1, -1, -1]\n",
            "Node Colors: [1, 0, 2, -1, -1]\n",
            "Node Colors: [1, 0, 2, 2, -1]\n",
            "Node Colors: [1, 0, 2, 2, -1]\n",
            "\n",
            "Final Colors: [1, 0, 2, 2, 0]\n",
            "Total Reward: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GraphColorQNet(torch.nn.Module):\n",
        "    def __init__(self, num_of_features, hidden_dim=64, color_embedding_dim=64, num_colors=3):\n",
        "        super().__init__()\n",
        "        self.gnn1 = GCNConv(num_of_features, hidden_dim)\n",
        "        self.gnn2 = GCNConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.color_embedding = torch.nn.Embedding(num_colors, color_embedding_dim)\n",
        "\n",
        "        self.q_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim + color_embedding_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        # data.x shape: [num_nodes, num_of_features (only colour for now)]\n",
        "        x = data.x.float()  # make sure it's float\n",
        "        edge_index = data.edge_index\n",
        "\n",
        "        h = self.gnn1(x, edge_index).relu()\n",
        "        h = self.gnn2(h, edge_index).relu()\n",
        "        return h  # node embeddings\n",
        "\n",
        "    def get_q_values(self, node_embs, node_ids, color_ids):\n",
        "        node_vecs = node_embs[node_ids]\n",
        "        color_embeddings = self.color_embedding(color_ids)\n",
        "\n",
        "        combined = torch.cat([node_vecs, color_embeddings], dim=-1)\n",
        "        q_values = self.q_mlp(combined)\n",
        "        return q_values\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YC0TG74XiVit"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "def select_action(model, data, num_colors, epsilon=0.1):\n",
        "    color_column = data.x[:, 0]\n",
        "    uncolored_nodes = torch.where(color_column == -1)[0]\n",
        "\n",
        "    if not len(uncolored_nodes):\n",
        "      print(\"GRAPH COLORED!\")\n",
        "      return None\n",
        "\n",
        "    colors = torch.arange(num_colors, device=data.x.device)\n",
        "    node_ids, color_ids = torch.meshgrid(uncolored_nodes, colors, indexing='ij')\n",
        "    node_ids = node_ids.flatten()\n",
        "    color_ids = color_ids.flatten()\n",
        "\n",
        "    # Step 3: Predict Q-values\n",
        "    with torch.no_grad():\n",
        "        node_embs = model(data)\n",
        "        q_values = model.get_q_values(node_embs, node_ids, color_ids)\n",
        "\n",
        "    # Step 4: Epsilon-greedy selection\n",
        "    if random.random() < epsilon:\n",
        "        # Randomly explore\n",
        "        idx = random.randint(0, len(q_values) - 1)\n",
        "    else:\n",
        "        # Exploit: pick best Q-value\n",
        "        idx = torch.argmax(q_values).item()\n",
        "\n",
        "    selected_node = node_ids[idx].item()\n",
        "    selected_color = color_ids[idx].item()\n",
        "    selected_q = q_values[idx].item()\n",
        "\n",
        "    return selected_node, selected_color, selected_q\n"
      ],
      "metadata": {
        "id": "FdhMCJajrrC_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GraphColoringEnv(num_colors=3)\n",
        "data, _ = env.reset()  # PyG Data object\n",
        "\n",
        "model = GraphColorQNet(num_of_features=1, hidden_dim=64, color_embedding_dim=64, num_colors=3)\n",
        "\n",
        "# Step 3: Get node embeddings\n",
        "node_embs = model(data)  # shape: [num_nodes, hidden_dim]\n",
        "\n",
        "node_ids = torch.tensor([0, 1, 2])     # Which nodes to color\n",
        "color_ids = torch.tensor([0, 1, 2])    # Try different colors\n",
        "q_values = model.get_q_values(node_embs, node_ids, color_ids)\n",
        "\n",
        "print(\"Node IDs:   \", node_ids.tolist())\n",
        "print(\"Color IDs:  \", color_ids.tolist())\n",
        "print(\"Q-values:   \", q_values.tolist())\n",
        "\n",
        "obs, _ = env.reset()\n",
        "action = select_action(model, obs, num_colors=3, epsilon=1)\n",
        "\n",
        "if action:\n",
        "    node, color, q = action\n",
        "    print(f\"Selected action: Color node {node} with color {color} (Q={q:.3f})\")\n",
        "else:\n",
        "    print(\"No valid actions left.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZZTXEEntpMA",
        "outputId": "63add05f-d227-43a9-8228-9dbb7602e2fd"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node IDs:    [0, 1, 2]\n",
            "Color IDs:   [0, 1, 2]\n",
            "Q-values:    [[-0.04053190350532532], [-0.015579842031002045], [0.22975106537342072]]\n",
            "Selected action: Color node 3 with color 1 (Q=-0.016)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One episode"
      ],
      "metadata": {
        "id": "lpzyl2t1zKef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment and reset\n",
        "env = GraphColoringEnv(num_colors=3)\n",
        "data, _ = env.reset()\n",
        "\n",
        "# Create model (fresh/random for now)\n",
        "model = GraphColorQNet(num_of_features=1, hidden_dim=64, color_embedding_dim=64, num_colors=3)\n",
        "\n",
        "# Episode parameters\n",
        "epsilon = 0.5\n",
        "done = False\n",
        "total_reward = 0\n",
        "step_count = 0\n",
        "\n",
        "print(\"Starting Episode...\\n\")\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "\n",
        "    # Select an action\n",
        "    action = select_action(model, data, num_colors=env.num_colors, epsilon=epsilon)\n",
        "\n",
        "    if action is None:\n",
        "        print(\"â— No valid actions left.\")\n",
        "        break\n",
        "\n",
        "    node, color, q_val = action\n",
        "    print(f\"-- Step {step_count}: Color node {node} with color {color} (Q={q_val:.3f})\")\n",
        "\n",
        "    # Apply action in environment\n",
        "    obs, reward, done, _, _ = env.step((node, color))\n",
        "    data = obs\n",
        "\n",
        "    total_reward += reward\n",
        "    step_count += 1\n",
        "    print(f\"--- Reward: {reward}\\n\")\n",
        "\n",
        "print(\"Episode finished.\")\n",
        "print(f\"Total reward: {total_reward}\")\n",
        "print(f\"Steps taken: {step_count}\")\n",
        "print(f\"Solution:\")\n",
        "env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2BPSf4wzJz9",
        "outputId": "6ec67ef1-607e-4799-a074-bcc50396dd77"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Episode...\n",
            "\n",
            "Node Colors: [-1, -1, -1, -1, -1]\n",
            "-- Step 0: Color node 4 with color 2 (Q=0.122)\n",
            "--- Reward: 1\n",
            "\n",
            "Node Colors: [-1, -1, -1, -1, 2]\n",
            "-- Step 1: Color node 0 with color 2 (Q=0.122)\n",
            "--- Reward: 1\n",
            "\n",
            "Node Colors: [2, -1, -1, -1, 2]\n",
            "-- Step 2: Color node 3 with color 0 (Q=0.003)\n",
            "--- Reward: 1\n",
            "\n",
            "Node Colors: [2, -1, -1, 0, 2]\n",
            "-- Step 3: Color node 1 with color 2 (Q=0.129)\n",
            "--- Reward: -1\n",
            "\n",
            "Node Colors: [2, -1, -1, 0, 2]\n",
            "-- Step 4: Color node 2 with color 0 (Q=-0.004)\n",
            "--- Reward: 1\n",
            "\n",
            "Node Colors: [2, -1, 0, 0, 2]\n",
            "-- Step 5: Color node 1 with color 0 (Q=0.008)\n",
            "--- Reward: -1\n",
            "\n",
            "Node Colors: [2, -1, 0, 0, 2]\n",
            "-- Step 6: Color node 1 with color 0 (Q=0.008)\n",
            "--- Reward: -1\n",
            "\n",
            "Node Colors: [2, -1, 0, 0, 2]\n",
            "-- Step 7: Color node 1 with color 1 (Q=-0.001)\n",
            "--- Reward: 11\n",
            "\n",
            "Episode finished.\n",
            "Total reward: 12\n",
            "Steps taken: 8\n",
            "Solution:\n",
            "Node Colors: [2, 1, 0, 0, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replay buffer + training"
      ],
      "metadata": {
        "id": "fRVBnzre3Zcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, node_id, color_id, reward, next_state, done):\n",
        "        self.buffer.append((state, node_id, color_id, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, node_ids, color_ids, rewards, next_states, dones = zip(*batch)\n",
        "        return list(states), torch.tensor(node_ids), torch.tensor(color_ids), \\\n",
        "               torch.tensor(rewards, dtype=torch.float32), list(next_states), torch.tensor(dones, dtype=torch.bool)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "\n",
        "def train_step(model, target_model, buffer, optimizer, batch_size, gamma, num_colors):\n",
        "    if len(buffer) < batch_size:\n",
        "        return None  # not enough data\n",
        "\n",
        "    states, node_ids, color_ids, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "\n",
        "    # Get Q(s, a)\n",
        "    q_vals = []\n",
        "    for i in range(batch_size):\n",
        "        node_embs = model(states[i])\n",
        "        q_val = model.get_q_values(node_embs, node_ids[i].unsqueeze(0), color_ids[i].unsqueeze(0))\n",
        "        q_vals.append(q_val)\n",
        "    q_vals = torch.stack(q_vals).squeeze()\n",
        "\n",
        "    # Compute target Q-values\n",
        "    target_qs = []\n",
        "    for i in range(batch_size):\n",
        "        if dones[i]:\n",
        "            target_q = rewards[i]\n",
        "        else:\n",
        "            next_node_embs = target_model(next_states[i])\n",
        "            color_space = torch.arange(num_colors)\n",
        "            next_node_ids, next_color_ids = torch.meshgrid(\n",
        "                torch.where(next_states[i].x[:, 0] == -1)[0],\n",
        "                color_space,\n",
        "                indexing='ij'\n",
        "            )\n",
        "            next_node_ids = next_node_ids.flatten()\n",
        "            next_color_ids = next_color_ids.flatten()\n",
        "            if len(next_node_ids) == 0:\n",
        "                max_q = 0.0\n",
        "            else:\n",
        "                q_vals_next = target_model.get_q_values(next_node_embs, next_node_ids, next_color_ids)\n",
        "                max_q = torch.max(q_vals_next)\n",
        "            target_q = rewards[i] + gamma * max_q\n",
        "        target_qs.append(target_q)\n",
        "    target_qs = torch.stack(target_qs)\n",
        "\n",
        "    # Loss & backprop\n",
        "    loss = torch.nn.functional.mse_loss(q_vals, target_qs)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n"
      ],
      "metadata": {
        "id": "tnvRX6tB3bah"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ],
      "metadata": {
        "id": "IIvwiqh232ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 300\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.05\n",
        "epsilon_decay = 0.999\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "learning_rate = 1e-3\n",
        "target_update_freq = 10"
      ],
      "metadata": {
        "id": "2caYRVaU3juY"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GraphColoringEnv(num_colors=3)\n",
        "model = GraphColorQNet(num_of_features=1, hidden_dim=64, color_embedding_dim=64, num_colors=3)\n",
        "target_model = GraphColorQNet(num_of_features=1, hidden_dim=64, color_embedding_dim=64, num_colors=3)\n",
        "target_model.load_state_dict(model.state_dict())  # start with same weights\n",
        "\n",
        "buffer = ReplayBuffer(capacity=10000)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "iXM6NV3e36hC"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = epsilon_start\n",
        "\n",
        "for episode in range(1, num_episodes + 1):\n",
        "    data, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    step_count = 0\n",
        "\n",
        "    while not done:\n",
        "        # Select action using current model\n",
        "        action = select_action(model, data, num_colors=env.num_colors, epsilon=epsilon)\n",
        "\n",
        "        if action is None:\n",
        "            break\n",
        "\n",
        "        node, color, _ = action\n",
        "        print(\"Env: \")\n",
        "        env.render()\n",
        "        print(f\"-- Selected node {node} with color {color}\")\n",
        "\n",
        "        # Save old state\n",
        "        state = data\n",
        "\n",
        "        # Step in env\n",
        "        next_data, reward, done, _, _ = env.step((node, color))\n",
        "        print(f\"- Reward for it: {reward}\")\n",
        "        total_reward += reward\n",
        "\n",
        "        # Save to buffer\n",
        "        buffer.add(state, node, color, reward, next_data, done)\n",
        "\n",
        "        # Update model\n",
        "        loss = train_step(model, target_model, buffer, optimizer, batch_size, gamma, num_colors=env.num_colors)\n",
        "\n",
        "        # Move to new state\n",
        "        data = next_data\n",
        "        step_count += 1\n",
        "\n",
        "    # Epsilon decay\n",
        "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "\n",
        "    # Update target model\n",
        "    if episode % target_update_freq == 0:\n",
        "        target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    # Logging\n",
        "    print(f\"ðŸŸ¢ Episode {episode:3d} | Steps: {step_count:2d} | Total Reward: {total_reward:3.0f} | Epsilon: {epsilon:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C3eaFGmK3_JZ",
        "outputId": "55a6123f-74fd-47db-eabb-c638892f4c6e"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Env: \n",
            "Node Colors: [-1, -1, -1, -1, -1]\n",
            "-- Selected node 3 with color 2\n",
            "- Reward for it: 1\n",
            "Env: \n",
            "Node Colors: [-1, -1, -1, 2, -1]\n",
            "-- Selected node 2 with color 1\n",
            "- Reward for it: 1\n",
            "Env: \n",
            "Node Colors: [-1, -1, 1, 2, -1]\n",
            "-- Selected node 0 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [-1, -1, 1, 2, -1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [-1, -1, 1, 2, -1]\n",
            "-- Selected node 4 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [-1, -1, 1, 2, -1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [-1, -1, 1, 2, -1]\n",
            "-- Selected node 0 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [-1, -1, 1, 2, -1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [-1, -1, 1, 2, -1]\n",
            "-- Selected node 4 with color 1\n",
            "- Reward for it: 1\n",
            "Env: \n",
            "Node Colors: [-1, -1, 1, 2, 1]\n",
            "-- Selected node 0 with color 0\n",
            "- Reward for it: 1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 1\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 2\n",
            "- Reward for it: -1\n",
            "Env: \n",
            "Node Colors: [0, -1, 1, 2, 1]\n",
            "-- Selected node 1 with color 0\n",
            "- Reward for it: -1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-97dcc9c559f6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Update model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_colors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_colors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Move to new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-7cb97a5ec920>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, target_model, buffer, optimizer, batch_size, gamma, num_colors)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtarget_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mnext_node_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mcolor_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_colors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             next_node_ids, next_color_ids = torch.meshgrid(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-6076897af8e5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m  \u001b[0;31m# node embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_edge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                     edge_index, edge_weight = gcn_norm(  # yapf: disable\n\u001b[0m\u001b[1;32m    242\u001b[0m                         \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                         self.improved, self.add_self_loops, self.flow, x.dtype)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mdeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mdeg_inv_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeg_inv_sqrt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0medge_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0medge_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LYdxIiMA3_Bs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}